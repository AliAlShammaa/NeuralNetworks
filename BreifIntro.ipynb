{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "In this notebook, I will explain my understanding of computation and intelligence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### The levels to computation\n",
    "1. Algorithmic Execution where inputs are mapped to output as defined by the program\n",
    "2. The Computer is given inputs and learns to map them to their given outputs. Eventually   \n",
    "      it can map any input to the corrrect output. This can be done with different machine  \n",
    "      learning algorithms. The key goal is to make right descions. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can choose among different kinds of NNs as model.\n",
    "\n",
    "#### The Perceptron\n",
    "We can imagine a computing unit that takes in binary input(s) and maps it to binary output.  \n",
    "The unit would multiply the input by their respective weights and adds a bias. Depending on   \n",
    "a threshold, it would then output either 0 or 1. The lower the threshold the higher our  \n",
    "chances are of getting 1. We can then define a bias = -threshold which would be such that  \n",
    "we get 0 if w * x + b <= 0 and 1 ow. The bias would define the tendency of the Perceptron  \n",
    "to output 1. One can implement NAND Gates with Perceptron and since NAND are universal for  \n",
    "computation, then so are perceptron. The unviersality theorem of ANNs states that given that  \n",
    "the activation function satsifies some set of requirements, then any continuous function can  \n",
    "be approximated by an ANN of the right size and with correct hyperparameters.  \n",
    "The question, what sort of algorithms is done so that model learns ?  \n",
    "\n",
    "In feedforward models, the input is not reliant on the output.\n",
    "\n",
    "#### Sigmoid Neuron\n",
    "Perceptrons have a discontinuous range and that would make it hard to learn. If replaced by  \n",
    "sigmoid neuron then they have continuous range and their output is a probability of it being  \n",
    "correct. It is easier to teach such ANN.\n",
    "\n",
    "##### How many neurons in the ouptut layer ?\n",
    "Increasing the number of neurons in the output layer so that they match the number of labels  \n",
    "ends up improving the results because each neuron can devote itself to change its activation  \n",
    "in the direction that would make the cost less when it has to fire. Chances that the different  \n",
    "labels do not match their inputs for any good reason (for eg: digits). So in the MNIST problem  \n",
    "if we have 4 instead of 10, then the first output layer will have to highly weight features  \n",
    "that pretain to all digits for which it fires."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}